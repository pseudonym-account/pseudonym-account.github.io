<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="RL, MOP, LAP, SAC">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Dynamic Reference RL</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  
</nav>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Unsupervised Behavioral Tokenization and Action Quantization via Maximum Entropy Mixture Policies with Minimum Entropy Components</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="">Anonymous Author</a></span> 
              <!-- Yamen Habib, Dmytro Grytskyy, Rubén Moreno-Bote -->
          </div>

          <!-- <div class="is-size-5 publication-authors">
            <span class="author-block">UPF</span>
          </div> 

          <div class="column has-text-centered"> 
            <div class="publication-links">
              <span class="link-block">
                <a href="https://openreview.net/pdf?id=HhbHw2yInZ"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
              
              <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a>
              </span> 
            </div> 
          </div>  -->

        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Most reinforcement learning approaches optimize behavior to maximize a taskspecific reward. However, from this, it is difficult to learn transferable token-like
            behaviors that can be reused and composed to solve arbitrary downstream tasks.
            We introduce an online unsupervised reinforcement learning framework that autonomously quantizes the agent’s action space into component policies via a joint
            entropy objective—maximizing the cumulative entropy of an overall mixture policy
            to ensure diverse, exploratory behavior under the maximum-occupancy principle,
            while minimizing the entropy of each component to enforce diversity and high
            specialization. Unlike existing approaches, our framework tackles action quantization into state-dependent component policies in a fully principled, unsupervised,
            online manner. We prove convergence in the tabular setting through a novel policy
            iteration algorithm, then extend to continuous control by fixing the discovered
            components and deploying them deterministically within an online optimizer to
            maximize cumulative reward. Empirical results demonstrate that our maxi-mixmini-com entropy-based action-policy quantization provides interpretable, reusable
            token-like behavioral patterns, yielding a fully online, task-agnostic, scalable architecture that requires no task-specific offline data and transfers readily across tasks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-justified">
            <!-- <h2 class="title is-3 has-text-centered">Overview</h2> -->
            <style>
            .overview-image {
              display: block;
              margin-left: auto;
              margin-right: auto;
                width: 60vw;
                max-width: 100vw;
                height: auto;
              }
              </style>
              <img class="overview-image" src="static/images/intro.PNG" alt="Overview figure" style="width:60vw;max-width:100vw;height:auto;">
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section id="videos_ant_det_vs_stoc" class="section">
  <div class="container ">
    <h2 class="title is-3 has-text-centered">Behavior under maxi-mix-mini-com</h2>
    
    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <h2 class="title is-4 has-text-centered"> \(k \sim w(s), \quad a \sim \pi_k(\cdot|s)\) </h2>
          <style>
            #videos_ant_det_vs_stoc p {
              text-align: justify;
            }
          </style>
          <p>
            The agent samples a component \(k\) from a state-dependent categorical distribution \(w(s)\) at each time step, then sample an action from component policy \(\pi_k\). 
          </p>

          <video id="video_sac" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/ant_agent_video_f20250723_002513_compressed.mp4"
                    type="video/mp4">
          </video>
          
        </div>
      </div>

      <div class="column">
        <h2 class="title is-4 has-text-centered"> \(k \sim Uniform \{0, ..., K\}, \quad a = \mu_k\)</h2>
        <div class="columns is-centered">
          <div class="column content">
            <style>
            #videos_ant_det_vs_stoc p {
              text-align: justify;
            }
            </style>
            <p>
              The agent samples a component \(k\) uniformly at random from \(\{0, ..., K\}\) at each time step, then use the mode \( \mu_k \) of component policy \(\pi_k\) as quantized action. 
            </p> 
            <video id="video_mop" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/ant_agent_video_uniform_random_component_20250723_004920_compressed.mp4"
                      type="video/mp4">
            </video>
            
          </div>

        </div>
      </div>
    </div>
  </div>
</section> 

<section id="videos_ant" class="section">
  <div class="container ">
    <h2 class="title is-3 has-text-centered">Ant-v5 Agent Dynamics Using A Fixed Component</h2>
    <p>
      This section evaluates the behavior of the Ant-v5 agent when consistently utilizing a single, fixed component policy \(\pi_k\) from the learned mixture policy. 
      By isolating and deploying one component at a time, we can observe the distinct behaviors and strategies that each component has specialized in during training. 
      This analysis provides insights into how different components contribute to the overall performance and adaptability of the agent in navigating its environment.
    </p>
    <br>
    <br>
    <div class="columns is-centered">

      <div class="column">
        <div class="content">
          <h4 class="title is-4  has-text-centered"> \( a = \mu_k \) </h4>
          <p style="text-align: justify;">
            The agent consistently uses the mode (most probable action) of a selected component policy \(\pi_k\) at each time step, resulting in deterministic behavior.
          </p>

          <video id="video_sac" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/grid_video_white_det_alpha_0_1.mp4"
                    type="video/mp4">
          </video>
          <!-- <p>
            Incorporates entropy regularization into its objective function to balance exploration and exploitation. 
          </p> -->
        </div>
      </div>

      <div class="column">
        <h4 class="title is-4  has-text-centered">\( a \sim \pi_k = \mathcal{N}(\mu_k(s), \Sigma_k(s))\)</h4>
        <div class="columns is-centered">
          <div class="column content">
            <p style="text-align: justify;">
              The agent samples actions from the component policy \(\pi_k = \mathcal{N}(\mu_k(s), \Sigma_k(s))\), where the diagonal elements of the covariance matrix \(\Sigma_k(s)\) are limited from above by \(0.1\).
            </p>
            <video id="video_mop" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/grid_video_white_std_0_1_alpha_0_1.mp4"
                      type="video/mp4">
            </video>
            <!-- <p>
              Maximizes the occupancy of action-state space by maximizing cumulative action entropy
            </p>  -->
          </div>

        </div>
      </div>
    </div>
  </div>
</section> 
  <section id="videos_swimmer_fixed_component" class="section">
    <div class="container ">
      <h2 class="title is-3 has-text-centered">Swimmer-v5 Agent Dynamics Using A Fixed Component</h2>
      <p>
        This section evaluates the behavior of the Swimmer-v5 agent when consistently utilizing a single, fixed component policy \(\pi_k\) from the learned mixture policy.
        By isolating and deploying one component at a time, we can observe the distinct behaviors and strategies that each component has specialized in during training.
        This analysis provides insights into how different components contribute to the overall performance and adaptability of the agent in navigating its environment.
      </p>
      <br>
      <br>
      <div class="columns is-centered">

        <div class="column">
          <div class="content">
            <h4 class="title is-4 has-text-centered"> \( a = \mu_k \) </h4>
            <p style="text-align: justify;">
              The agent consistently uses the mode (most probable action) of a selected component policy \(\pi_k\) at each time step, resulting in deterministic behavior.
            </p>
            <video id="video_swimmer_det" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/swimmer_std_0_0.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <h4 class="title is-4 has-text-centered">\( a \sim \pi_k = \mathcal{N}(\mu_k(s), \Sigma_k(s))\)</h4>
          <div class="columns is-centered">
            <div class="column content">
              <p style="text-align: justify;">
                The agent samples actions from the component policy \(\pi_k = \mathcal{N}(\mu_k(s), \Sigma_k(s))\), where the diagonal elements of the covariance matrix \(\Sigma_k(s)\) are limited from above by \(0.1\).
              </p>
              <video id="video_swimmer_stoc" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/Swimmer_std_0_1.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>


  <section id="videos_fetch_fixed_component" class="section">
    <div class="container ">
      <h2 class="title is-3 has-text-centered">FetchReach-v5 Agent Dynamics Using A Fixed Component</h2>
      <p>
        This section evaluates the behavior of the FetchReach-v5 agent when consistently utilizing a single, fixed component policy \(\pi_k\) from the learned mixture policy.
        By isolating and deploying one component at a time, we can observe the distinct behaviors and strategies that each component has specialized in during training.
        This analysis provides insights into how different components contribute to the overall performance and adaptability of the agent in navigating its environment.
      </p>
      <br>
      <br>
      <div class="columns is-centered">

        <div class="column">
          <div class="content">
            <h4 class="title is-4 has-text-centered"> \( a = \mu_k \) </h4>
            <p style="text-align: justify;">
              The agent consistently uses the mode (most probable action) of a selected component policy \(\pi_k\) at each time step, resulting in deterministic behavior.
            </p>
            <video id="video_fetch_det" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/fetchreach_std_0_0.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>

        <div class="column">
          <h4 class="title is-4 has-text-centered">\( a \sim \pi_k = \mathcal{N}(\mu_k(s), \Sigma_k(s))\)</h4>
          <div class="columns is-centered">
            <div class="column content">
              <p style="text-align: justify;">
                The agent samples actions from the component policy \(\pi_k = \mathcal{N}(\mu_k(s), \Sigma_k(s))\), where the diagonal elements of the covariance matrix \(\Sigma_k(s)\) are limited from above by \(0.1\).
              </p>
              <video id="video_fetch_stoc" autoplay controls muted loop playsinline height="100%">
                <source src="./static/videos/fetchreach_std_0.1.mp4"
                        type="video/mp4">
              </video>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>



<section id="videos_random" class="section">
  <div class="container ">
    <div class="content has-text-justified">
      <h2 class="title is-3" style="text-align: justify;">Comparison between our method and using random set of actions</h2>
      <p>
      This section evaluates our method&#8217;s ability to control the complex dynamics of the MuJoCo Humanoid and Walker2d tasks, where randomly generated actions invariably fail. 
      <br> 
      For example, in the default Humanoid-v5 environment, episodes terminate if the torso&#8217;s z&#8209;coordinate (height) falls outside the healthy interval \\(1.0,\,2.0\\). 
      We compare our approach against two random&#8209;action baselines: (1) discretized sampling, in which K actions are generated by drawing each action dimension independently from {&minus;magnitude, 0, + magnitude}&mdash;with magnitudes of 0.05, 0.1, 0.2, 0.3, and 0.4 and K equal to 4, 8, 16, 32, 64, or 128&mdash;and (2) uniform sampling across the continuous action space. Across every tested magnitude and number of generated actions, neither baseline produces sustained or meaningful Humanoid behaviors under this termination criterion.
      </p>
    </div>
    
    <div id="videos_random_humanoid" class="columns is-centered">

      <div class="column">
        <div class="content">
          <h4 class="title is-4 is-centered has-text-centered"> Maxi-Mix-Mini-Com </h4>
          <video id="video_humanoid_agent" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Humanoid-v5_agent_video_1_compressed.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <div  class="column">
        <h4 class="title is-4 is-centered has-text-centered">Random Actions</h4>
        <div class="columns is-centered">
          <div class="column content">
            <video id="video_humanoid_random" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/Humanoid-v5_comprehensive_analysis_1_compressed.mp4"
                      type="video/mp4">
            </video>
          </div>

        </div>
      </div>
    </div>

    <div id="videos_random_walker" class="columns is-centered">

      <div class="column">
        <div class="content">
          <h4 class="title is-4 is-centered has-text-centered"> Maxi-Mix-Mini-Com </h4>
          <video id="video_walker_agent" autoplay controls muted loop playsinline height="100%">
            <source src="./static/videos/Walker2d-v5_agent_video_f20250722_094445_compressed.mp4"
                    type="video/mp4">
          </video>
        </div>
      </div>

      <div class="column">
        <h4 class="title is-4 is-centered has-text-centered">Random Actions</h4>
        <div class="columns is-centered">
          <div class="column content">
            <video id="video_walker_random" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/Walker2d-v5_comprehensive_analysis_1_compressed.mp4"
                      type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>



  </div>
</section>


<section id="videos_humanoid_mapping" class="section">
  <div class="container ">
    <div class="content has-text-centered">
      <h2 class="title is-3">Mapping from State to Components</h2>
    </div>
    <div class="content is-centered">
      <div class="columns is-centered">
        <div class="column content is-centered">

          <div style="max-width: 800px; margin: 0 auto;">
            <p>
              This section visualizes the state-to-component mapping learned by the mixture policy in the Humanoid-v5 environment. 
              We can see how our mixture policy has high entropic mixing weights with the low entropic components specialized in different behaviors.
            </p>
            <br>
          </div>
          <video style="display: block; margin: 0 auto;" id="video_humanoid_agent" autoplay controls muted loop playsinline height="50%" width="50%">
            <source src="./static/videos/Humanoid-v5-agent_praw_video_compressed.mp4" type="video/mp4">
          </video>
        </div>
      </div>
    </div> 
  </div>
  
  <section id="videos_ant_metra_comparison" class="section">
    <div class="container ">
      <div class="content has-text-centered">
        <h2 class="title is-3">Comparison: METRA vs. Our Method (Ant-v5, Fixed Component)</h2>
        <p>
          This section compares the behaviors discovered by the METRA skill discovery method and our maxi-mix-mini-com approach in the Ant-v5 environment, using both top and side camera views. 
          We are showing 16 components for our mixture against a trained METRA model on 16 dimensions discrete latent skill variables.
        </p>
      </div>
      <br>
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-centered">
            <h4 class="title is-4">METRA</h4>
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/videos/ant_kl_com_metra_top_camera_compressed.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column">
          <div class="content has-text-centered">
            <h4 class="title is-4">Ours</h4>
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/videos/ant_kl_com_Ours_top_camera_compressed.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
      <br>
      <div class="columns is-centered">
        <div class="column">
          <div class="content has-text-centered">
            <h4 class="title is-4">METRA</h4>
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/videos/ant_kl_com_metra_side_camera_compressed.mp4" type="video/mp4">
            </video>
          </div>
        </div>
        <div class="column">
          <div class="content has-text-centered">
            <h4 class="title is-4">Ours</h4>
            <video autoplay controls muted loop playsinline width="100%">
              <source src="static/videos/ant_kl_com_Ours_side_camera_compressed.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>
</section>

    

<!-- <section class="section">
  <div class="container ">
    <h2 class="title is-3 has-text-centered">Mujoco Benchmark</h2>
    
    <div class="content has-text-centered"">
      
    <table>
      <thead>
          <tr>
              <th>Metric/Method</th>
              <th>SAC</th>
              <th>PPO</th>
              <th>TD3</th>
              <th>DDPG</th>
              <th>MOP reference</th>
              <th>Lap reference</th>
          </tr>
      </thead>
      <tbody>
          <tr>
              <td>Ant-v4 500k</td>
              <td>5754</td>
              <td>1431</td>
              <td>3857</td>
              <td>-521</td>
              <td><strong>6166</strong></td>
              <td>6016</td>
          </tr>
          <tr>
              <td>Ant-v4 1M</td>
              <td>6427</td>
              <td>1929</td>
              <td>4378</td>
              <td>413</td>
              <td>6647</td>
              <td><strong>6695</strong></td>
          </tr>
          <tr>
              <td>Hum-v4 500k</td>
              <td>4761</td>
              <td>546</td>
              <td>93</td>
              <td>130</td>
              <td>4972</td>
              <td><strong>5294</strong></td>
          </tr>
          <tr>
              <td>Hum-v4 1M</td>
              <td>5316</td>
              <td>661</td>
              <td>93</td>
              <td>127</td>
              <td>5075</td>
              <td><strong>5489</strong></td>
          </tr>
          <tr>
              <td>Walker2d-v4 500k</td>
              <td>3347</td>
              <td>2781</td>
              <td>3587</td>
              <td>396</td>
              <td>3656</td>
              <td><strong>3919</strong></td>
          </tr>
          <tr>
              <td>Walker2d-v4 1M</td>
              <td>4297</td>
              <td>3444</td>
              <td>4172</td>
              <td>1449</td>
              <td>4173</td>
              <td><strong>4558</strong></td>
          </tr>
          <tr>
              <td>Hopper-v4 500k</td>
              <td>2292</td>
              <td>961</td>
              <td><strong>3204</strong></td>
              <td>1450</td>
              <td>2209</td>
              <td>1691</td>
          </tr>
          <tr>
              <td>Hopper-v4 1M</td>
              <td>2766</td>
              <td>935</td>
              <td><strong>3294</strong></td>
              <td>1721</td>
              <td>2784</td>
              <td>2394</td>
          </tr>
          <tr>
              <td>Cheetah-v4 500k</td>
              <td>8048</td>
              <td>4840</td>
              <td>8777</td>
              <td><strong>9968</strong></td>
              <td>8348</td>
              <td>9098</td>
          </tr>
          <tr>
              <td>Cheetah-v4 1M</td>
              <td>9378</td>
              <td>5463</td>
              <td>10304</td>
              <td><strong>11607</strong></td>
              <td>9702</td>
              <td>10516</td>
          </tr>
          <tr>
              <td>mean</td>
              <td>5238</td>
              <td>2299</td>
              <td>4175</td>
              <td>2674</td>
              <td><strong>5373</strong></td>
              <td><strong>5567</strong></td>
          </tr>
      </tbody>
    </table>

    </div>
  </div>
</section>  -->

<!-- <section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column">
        <div class="content has-text-justified">
          <h2 class="title is-3 has-text-centered">Conclusion</h2>
        </div>
          
        <div class="content has-text-justified">
          <p>
            The findings from our experiments highlight that even state-of-the-art methods often struggle with directionless exploration or overcommitment to suboptimal policies. 
            Our ORDRP approach addresses these challenges by providing a reference policy that evolves in response to the learning environment. 
            This adaptive nature allows for more structured exploration, reducing the risk of inefficient learning trajectories. 
          </p>

          <p>
            Our study explored a novel approach to RL by introducing an off-reward dynamic reference policy to guide the main policy through complex learning environments. 
            This reference policy evolves alongside the target policy, allowing for more targeted exploration and reducing the inefficiencies of traditional exploration methods. 
            Our approach demonstrates that incorporating an off-reward reference policy into RL improves convergence rates and enhances performance in various tasks. 
            The off-reward functions that we used, based on MOP and graph Laplacian, demonstrated to be much more efficient in exploring, learning and optimizing target rewards in slightly more involved environments than the ones typically considered. 
          </p>

          <p>
            The experiments conducted in this project, including the Escape Room scenario and MuJoCo Benchmark Evaluation, demonstrate the success of our method. 
          </p>

        </div>
      </div>
    </div>
  </div>
</section> -->


<!-- 
<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<style>
  footer.footer {
    padding-top: 1rem;
    padding-bottom: 1rem;
    margin-top: 2rem;
    height: auto !important;
    min-height: unset !important;
  }
</style>
<footer class="footer">
  <div class="container">
    <!-- <div class="content has-text-centered">
       <a class="icon-link"
         href="https://openreview.net/pdf?id=HhbHw2yInZ">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/YamenHabib" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a> 
    </div> -->
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content has-text-centered">
          <p>
            The website template was borrowed from <a href="http://mgharbi.com/">Michaël Gharbi</a> and <a href="https://jonbarron.info/">Jon Barron</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
